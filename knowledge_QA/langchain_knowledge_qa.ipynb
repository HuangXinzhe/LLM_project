{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本地知识问答系统"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置openai api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ['base_url'] = \"https://api.fe8.cn/v1\"\n",
    "# api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# base_url = \"https://api.fe8.cn/v1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载并拆分数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the PDF using pypdf\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# load the data\n",
    "loader = PyPDFLoader('/book.pdf')\n",
    "\n",
    "# the 10k financial report are huge, we will need to split the doc into multiple chunk.\n",
    "# This text splitter is the recommended one for generic text. It is parameterized by a list of characters. \n",
    "# It tries to split on them in order until the chunks are small enough.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)  # 文本分割1000个字符为一个chunk，重叠0个字符\n",
    "data = loader.load()\n",
    "texts = text_splitter.split_documents(data)\n",
    "\n",
    "# view the first chunk\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本保存数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "# import Chroma and OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# initialize OpenAIEmbedding\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
    "\n",
    "# use Chroma to create in-memory embedding database from the doc\n",
    "docsearch = Chroma.from_documents(texts, embeddings,  metadatas=[{\"source\": str(i)} for i in range(len(texts))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本问答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## perform search based on the question\n",
    "query = \"What is the operating income?\"\n",
    "docs = docsearch.similarity_search(query)  # 找到和query最相似的文档"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 上述方法总结\n",
    "上述方法工作流程\n",
    "- 上述方法将输入的文本按照1000个字符进行切分\n",
    "- 将切分后的文本Embedding后保存到数据库\n",
    "- 通过输入的问题，将问题Embedding后与数据库中的文本进行相似度计算，找到最相似的文本\n",
    "\n",
    "上述方法存在的问题\n",
    "- 每个文本1000个字符，且每个文本之间没有重叠，容易产生每一个文本语义完成的文本，导致无法很好的回答问题\n",
    "- 上述方法中chromadb是临时数据库，如果后续需要则需要建立长期数据库\n",
    "- Embedding模型选用的是openai的Embedding模型，可以使用开源的Embedding模型进行替换，降低成本\n",
    "- 检索的方式仅使用了向量检索，检索的效果可能不是最好的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain提供了四种预先构建的问答Chain，具体如下：\n",
    "\n",
    "- 问答：load_qa_chain\n",
    "- 有来源问答：load_qa_with_sources_chain\n",
    "- 检索问题答案：RetrievalQA\n",
    "- 资源检索问答：RetrievalQAWithSourcesChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing necessary framework\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 问答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use LLM to get answering\n",
    "chain = load_qa_chain(ChatOpenAI(temperature=0.2,model_name='gpt-3.5-turbo'), \n",
    "                      chain_type=\"stuff\")\n",
    "query = \"What is the operating income?\"\n",
    "chain.run(input_documents=docs, question=query) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 有来源问答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_with_sources_chain(ChatOpenAI(temperature=0.2,model_name='gpt-3.5-turbo'), \n",
    "                                   chain_type=\"stuff\")\n",
    "query = \"What is the operating income?\"\n",
    "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 检索问题答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qa=RetrievalQA.from_chain_type(llm=ChatOpenAI(temperature=0.2,model_name='gpt-3.5-turbo'), chain_type=\"stuff\", \n",
    "                                                retriever=docsearch.as_retriever())\n",
    "query = \"What is the operating income?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 资源检索问答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=RetrievalQAWithSourcesChain.from_chain_type(ChatOpenAI(temperature=0.2,model_name='gpt-3.5-turbo'), chain_type=\"stuff\", \n",
    "                                                    retriever=docsearch.as_retriever())\n",
    "chain({\"question\": \"What is the operating income?\"}, return_only_outputs=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LlamaIndex\n",
    "- LlamaIndex是通过以下步骤将LLM连接到用户来响应查询的另一种方式（类似于Langchain的方式）：\n",
    "    - 加载文档（手动或通过数据加载程序）\n",
    "    - 将文档解析为节点\n",
    "    - 构造索引（从节点或文档）\n",
    "    - [可选，高级]在其他指数之上构建指数\n",
    "    - 查询索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "## setup your OpenAI Key\n",
    "# import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_OPENAI_API_KEY>\"\n",
    "# enable logs to see what happen underneath\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LlamaIndex的核心是指数，有多种类型的指数。\n",
    "- 列表索引\n",
    "- 矢量存储索引\n",
    "- 树索引\n",
    "- 关键字表索引\n",
    "- 图形索引\n",
    "- SQL索引。\n",
    "\n",
    "每个索引都有其独特的用途，具有不同的用途。好处是，您可以将索引堆叠在其他索引之上，这样做将使您的应用程序更强大，能够理解您的文档上下文和应用程序需求。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTVectorStoreIndex\n",
    "from llama_index import download_loader\n",
    "\n",
    "# we will use this UnstructuredReader to read PDF file\n",
    "UnstructuredReader = download_loader('UnstructuredReader', refresh_cache=True)\n",
    "loader = UnstructuredReader()\n",
    "# load the data\n",
    "data = loader.load_data(f'../notebooks/documents/_10-Q-Q1-2022-(As-Filed).pdf', split_documents=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document表示数据源的轻量级容器。可以选择下面两步骤之一：\n",
    "\n",
    "将Document对象直接输入索引\n",
    "\n",
    "首先，将文档转换为Node对象\n",
    "\n",
    "       同样，本系列的目的是帮助您尽快构建第一个应用程序，因此我将直接讨论索引构建。我将在未来的一篇文章中介绍LLamaIndex的所有方面。\n",
    "\n",
    "索引构建与查询\n",
    "\n",
    "       我们现在可以在这些Document对象上建立一个索引。最简单的高级抽象是在索引初始化期间加载Document对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = GPTVectorStoreIndex.from_documents(data)\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is the operating income?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据您使用的索引，LlamaIndex可能会进行LLM调用以构建索引。GPTVvectorStoreIndex不会调用LLM，但GPTTreeStoreIndex会调用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义LLM\n",
    "默认情况下，LlamaIndex使用OpenAI的text-davinci-003模型。在构造索引时，您可以选择使用另一个LLM。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import LLMPredictor, PromptHelper, ServiceContext\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# define LLM\n",
    "llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.2,model_name='gpt-3.5-turbo'))\n",
    "# define prompt helper\n",
    "# set maximum input size\n",
    "max_input_size = 4096\n",
    "# set number of output tokens\n",
    "num_output = 256\n",
    "# set maximum chunk overlap\n",
    "max_chunk_overlap = 20\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
    "index = GPTVectorStoreIndex.from_documents(\n",
    "    documents, \n",
    "    service_context=service_context\n",
    ")\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is the operating income?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完整示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# do imports\n",
    "from langchain.agents import Tool\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "from llama_index.langchain_helpers.agents import LlamaToolkit, create_llama_chat_agent, IndexToolConfig\n",
    "query_engine = index.as_query_engine()\n",
    "tool_config = IndexToolConfig(\n",
    "    query_engine=query_engine, \n",
    "    name=f\"Financial Report\",\n",
    "    description=f\"useful for when you want to answer queries about the Apple financial report\",\n",
    "    tool_kwargs={\"return_direct\": True}\n",
    ")\n",
    "toolkit = LlamaToolkit(\n",
    "    index_configs=[tool_config]\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "llm=ChatOpenAI(temperature=0.2,model_name='gpt-3.5-turbo')\n",
    "agent_chain = create_llama_chat_agent(\n",
    "    toolkit,\n",
    "    llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "while True:\n",
    "    text_input = input(\"User: \")\n",
    "    response = agent_chain.run(input=text_input)\n",
    "    print(f'Agent: {response}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
