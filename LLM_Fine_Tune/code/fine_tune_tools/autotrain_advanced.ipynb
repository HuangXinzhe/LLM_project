{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 微调工具autotrain-advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用autotrain-advanced微调LLAMA-2\n",
    "AutoTrain是一种无代码工具，用于为自然语言处理（NLP）任务、计算机视觉（CV）任务、语音任务甚至表格任务训练最先进的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install autotrain-advanced\n",
    "!pip install huggingface_hub\n",
    "\n",
    "# update torch\n",
    "!autotrain setup --update-torch\n",
    "\n",
    "\n",
    "# Login to huggingface\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!autotrain llm \\\n",
    "--train \\\n",
    "--model {MODEL_NAME} \\\n",
    "--project-name {PROJECT_NAME} \\\n",
    "--data-path data/ \\\n",
    "--text-column text \\\n",
    "--lr {LEARNING_RATE} \\\n",
    "--batch-size {BATCH_SIZE} \\\n",
    "--epochs {NUM_EPOCHS} \\\n",
    "--block-size {BLOCK_SIZE} \\\n",
    "--warmup-ratio {WARMUP_RATIO} \\\n",
    "--lora-r {LORA_R} \\\n",
    "--lora-alpha {LORA_ALPHA} \\\n",
    "--lora-dropout {LORA_DROPOUT} \\\n",
    "--weight-decay {WEIGHT_DECAY} \\\n",
    "--gradient-accumulation {GRADIENT_ACCUMULATION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "核心参数含义：\n",
    "\n",
    "— llm: 微调模型的类型\n",
    "\n",
    "— project_name: 项目名称\n",
    "\n",
    "— model: 需要微调的基础模型\n",
    "\n",
    "— data_path: 指定微调所需要的数据，可以使用huggingface上的数据集\n",
    "\n",
    "— text_column: 如果数据是表格，需要指定instructions和responses对应的列名\n",
    "\n",
    "— use_peft: 指定peft某一种方法\n",
    "\n",
    "— use_int4: 指定int 4量化\n",
    "\n",
    "— learning_rate: 学习率\n",
    "\n",
    "— train_batch_size: 训练批次大小\n",
    "\n",
    "— num_train_epochs: 训练轮数大小\n",
    "\n",
    "— trainer: 指定训练的方式\n",
    "\n",
    "— model_max_length: 设置模型最大上下文窗口\n",
    "\n",
    "— push_to_hub（可选）: 微调好的模型是否需要存储到Hugging Face? \n",
    "\n",
    "— repo_id: 如果要存储微调好的模型到Hugging Face，需要指定repository ID\n",
    "\n",
    "— block_size: 设置文本块大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!autotrain llm\n",
    "--train\n",
    "--project_name \"llama2-autotrain-openassitant\"\n",
    "--model TinyPixel/Llama-2-7B-bf16-sharded\n",
    "--data_path timdettmers/openassistant-guanaco\n",
    "--text_column text\n",
    "--use_peft\n",
    "--use_int4\n",
    "--learning_rate 0.4\n",
    "--train_batch_size 3\n",
    "--num_train_epochs 2\n",
    "--trainer sft\n",
    "--model_max_length 1048\n",
    "--push_to_hub\n",
    "--repo_id trojrobert/llama2-autotrain-openassistant\n",
    "--block_size 1048 > training.log"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
