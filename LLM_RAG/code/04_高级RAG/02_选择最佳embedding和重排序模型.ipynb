{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 选择最佳embedding和重排序模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检索评估指标\n",
    "命中率（Hit Rate）：\n",
    "- 命中率计算在前k个检索到的文档中找到正确答案的查询的百分比。简单地说，这是关于我们的系统在前几次猜测中正确的频率。\n",
    "\n",
    "平均倒数排名（MRR）：\n",
    "- 对于每个查询，MRR通过查看排名最高的相关文档的排名来评估系统的准确性。具体来说，它是所有查询中这些排名的倒数的平均值。因此，如果第一个相关文档是最高结果，则倒数为1；如果是第二个，则倒数为1/2，依此类推。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置环境、key和下载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index sentence-transformers cohere anthropic voyageai protobuf pypdf\n",
    "\n",
    "import openai\n",
    "openai_api_key = 'YOUR OPENAI API KEY'\n",
    "cohere_api_key = 'YOUR COHEREAI API KEY'\n",
    "anthropic_api_key = 'YOUR ANTHROPIC API KEY'\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "\n",
    "# 下载数据\n",
    "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"llama2.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "\n",
    "# LLM\n",
    "from llama_index.llms import Anthropic\n",
    "\n",
    "# Embeddings\n",
    "from llama_index.embeddings import OpenAIEmbedding, HuggingFaceEmbedding, CohereEmbedding\n",
    "from langchain.embeddings import VoyageEmbeddings, GooglePalmEmbeddings\n",
    "\n",
    "# Retrievers\n",
    "from llama_index.retrievers import (\n",
    "    BaseRetriever,\n",
    "    VectorIndexRetriever,\n",
    ")\n",
    "\n",
    "# Rerankers\n",
    "from llama_index.indices.query.schema import QueryBundle, QueryType\n",
    "from llama_index.schema import NodeWithScore\n",
    "from llama_index.indices.postprocessor.cohere_rerank import CohereRerank\n",
    "from llama_index.indices.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.finetuning.embeddings.common import EmbeddingQAFinetuneDataset\n",
    "\n",
    "# Evaluator\n",
    "from llama_index.evaluation import (\n",
    "    generate_question_context_pairs,\n",
    "    EmbeddingQAFinetuneDataset,\n",
    ")\n",
    "from llama_index.evaluation import RetrieverEvaluator\n",
    "\n",
    "\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import openai\n",
    "import voyageai\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(input_files=[\"llama2.pdf\"]).load_data()\n",
    "\n",
    "node_parser = SimpleNodeParser.from_defaults(chunk_size=512)  # 块大小为512\n",
    "nodes = node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成问题上下文对\n",
    "为了评估，我们创建了一个问题上下文对数据集，该数据集包括一系列问题及其相应的上下文。为了消除embedding（OpenAI/CohereAI）和重排序（CohereAI）评估的偏差，我们使用Anthropic LLM生成问题上下文对。\n",
    "\n",
    "初始化一个Prompt模板来生成问题上下文对。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt to generate questions\n",
    "qa_generate_prompt_tmpl = \"\"\"\\\n",
    "Context information is below.\n",
    "\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "\n",
    "Given the context information and not prior knowledge.\n",
    "generate only questions based on the below query.\n",
    "\n",
    "You are a Professor. Your task is to setup \\\n",
    "{num_questions_per_chunk} questions for an upcoming \\\n",
    "quiz/examination. The questions should be diverse in nature \\\n",
    "across the document. The questions should not contain options, not start with Q1/ Q2. \\\n",
    "Restrict the questions to the context information provided.\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Anthropic(api_key=anthropic_api_key)\n",
    "qa_dataset = generate_question_context_pairs(\n",
    "    nodes, llm=llm, num_questions_per_chunk=2\n",
    ")  # 这是一个生成问题的函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "过滤句子的函数，例如——以下是基于所提供上下文的两个问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean the dataset\n",
    "def filter_qa_dataset(qa_dataset):\n",
    "    \"\"\"\n",
    "    Filters out queries from the qa_dataset that contain certain phrases and the corresponding\n",
    "    entries in the relevant_docs, and creates a new EmbeddingQAFinetuneDataset object with\n",
    "    the filtered data.\n",
    "\n",
    "    :param qa_dataset: An object that has 'queries', 'corpus', and 'relevant_docs' attributes.\n",
    "    :return: An EmbeddingQAFinetuneDataset object with the filtered queries, corpus and relevant_docs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract keys from queries and relevant_docs that need to be removed\n",
    "    queries_relevant_docs_keys_to_remove = {\n",
    "        k for k, v in qa_dataset.queries.items()\n",
    "        if 'Here are 2' in v or 'Here are two' in v\n",
    "    }\n",
    "\n",
    "    # Filter queries and relevant_docs using dictionary comprehensions\n",
    "    filtered_queries = {\n",
    "        k: v for k, v in qa_dataset.queries.items()\n",
    "        if k not in queries_relevant_docs_keys_to_remove\n",
    "    }\n",
    "    filtered_relevant_docs = {\n",
    "        k: v for k, v in qa_dataset.relevant_docs.items()\n",
    "        if k not in queries_relevant_docs_keys_to_remove\n",
    "    }\n",
    "\n",
    "    # Create a new instance of EmbeddingQAFinetuneDataset with the filtered data\n",
    "    return EmbeddingQAFinetuneDataset(\n",
    "        queries=filtered_queries,\n",
    "        corpus=qa_dataset.corpus,\n",
    "        relevant_docs=filtered_relevant_docs\n",
    "    )\n",
    "\n",
    "# filter out pairs with phrases `Here are 2 questions based on provided context`\n",
    "qa_dataset = filter_qa_dataset(qa_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义检索器\n",
    "为了寻找最优检索器，采用embedding模型和重排序器的组合。最初建立了一个基本的VectorIndexRetriever。在检索节点后引入一个重排序器来进一步细化结果。值得注意的是，对于这个特定的实验，将similarity_top_k设置为10，并用reranker选择了前5名。但可以根据具体实验的需要随意调整此参数。在这里展示了OpenAIEmbedding的代码，其他embedding代码请参阅笔记本（https://colab.research.google.com/drive/1TxDVA__uimVPOJiMEQgP5fwHiqgKqm4-?usp=sharing）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = OpenAIEmbedding()\n",
    "service_context = ServiceContext.from_defaults(llm=None, embed_model = embed_model)\n",
    "vector_index = VectorStoreIndex(nodes, service_context=service_context)\n",
    "vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs both Vector search and Knowledge Graph search\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "\n",
    "        self._vector_retriever = vector_retriever\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "\n",
    "        retrieved_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "\n",
    "        if reranker != 'None':\n",
    "            retrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n",
    "        else:\n",
    "            retrieved_nodes = retrieved_nodes[:5]\n",
    "            \n",
    "        return retrieved_nodes\n",
    "\n",
    "    async def _aretrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Asynchronously retrieve nodes given query.\n",
    "\n",
    "        Implemented by the user.\n",
    "\n",
    "        \"\"\"\n",
    "        return self._retrieve(query_bundle)\n",
    "\n",
    "    async def aretrieve(self, str_or_query_bundle: QueryType) -> List[NodeWithScore]:\n",
    "        if isinstance(str_or_query_bundle, str):\n",
    "            str_or_query_bundle = QueryBundle(str_or_query_bundle)\n",
    "        return await self._aretrieve(str_or_query_bundle)\n",
    "\n",
    "custom_retriever = CustomRetriever(vector_retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估\n",
    "评估检索器，计算了平均倒数排名（MRR）和命中率指标："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    [\"mrr\", \"hit_rate\"], retriever=custom_retriever\n",
    ")\n",
    "eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
