{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用AutoGen、LangChian、RAG以及函数调用构建超级对话系统"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、安装环境及所需要的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain , \"pyautogen[retrievechat]\" , PyPDF2 , faiss-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、配置AutoGen和API密钥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoGen的配置文件是config_list\n",
    "config_list = [    \n",
    "    {        \n",
    "        \"model\": \"gpt-4-1106-preview\",        \n",
    "        \"api_key\": \"openai_api\",    \n",
    "    }\n",
    "    ]\n",
    "\n",
    "llm_config_proxy = {    \n",
    "    \"seed\": 42,  # change the seed for different trials    \n",
    "    \"temperature\": 0,    \n",
    "    \"config_list\": config_list,    \n",
    "    \"request_timeout\": 600\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、读取PDF文件\n",
    "- 上传一个PDF文件并进行处理，使用PyPDF2读取PDF文件；\n",
    "- 使用langchain中的text splitter将文本分割成chunk；\n",
    "- 使用OpenAIEmbeddings嵌入PDF文件，然后FAISS存储在向量数据库中；\n",
    "- Faiss可以将文本chunk转换为embedding。然后，这些向量可以用于各种应用，如相似性搜索。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader('/content/openchat.pdf')\n",
    "corpus = ''.join([p.extract_text() for p in reader.pages if p.extract_text()])  # 将读取的PDF文件转换为文本\n",
    "\n",
    "splitter =  RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200,)  # 将文本分割为1000个字符的块，每个块之间有200个字符的重叠\n",
    "chunks = splitter.split_text(corpus)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key = openai_api)\n",
    "vectors = FAISS.from_texts(chunks, embeddings)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5、会话检索\n",
    "- 使用Langchain的ConversationalRetrievalChain对用户的Prompt进行相似性搜索；\n",
    "- let call ConversationBufferMemory是一个简单的内存缓冲区，用于存储会话的历史记录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    OpenAI(temperature=0),    \n",
    "    vectors.as_retriever(),      # 使用FAISS向量存储库作为检索器\n",
    "    memory=ConversationBufferMemory(memory_key=\"chat_history\",     \n",
    "                                    return_messages=True),  # 使用ConversationBufferMemory作为内存\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6、指定Assistant代理配置\n",
    "AutoGen Agent支持对OpenAI模型的函数调用，但我们需要使用以下代码段指定函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config_assistant = {    \n",
    "    \"Seed\" : 42,    \n",
    "    \"temperature\": 0,        \n",
    "    \"functions\": [        \n",
    "        {            \n",
    "        \"name\": \"answer_PDF_question\",            \n",
    "        \"description\": \"Answer any PDF related questions\",            \n",
    "        \"parameters\": {                \n",
    "            \"type\": \"object\",                \n",
    "            \"properties\": {                    \n",
    "                \"question\": {                        \n",
    "                    \"type\": \"string\",                        \n",
    "                    \"description\": \"The question to ask in relation to PDF\",                    \n",
    "                    }                \n",
    "                },                \n",
    "                \"required\": [\"question\"],            \n",
    "            },                    \n",
    "        }    \n",
    "    ],\n",
    "    \"config_list\": config_list,   \n",
    "    \"timeout\": 120, \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7、配置Assistant Agent\n",
    "创建一个名为“assistant”的具有特定配置的自动化助理代理。我们使用该assistant阅读PDF并生成准确的答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = autogen.AssistantAgent(\n",
    "    name=\"assistant\",            \n",
    "    llm_config=llm_config_assistant,            \n",
    "    system_message=\"\"\"\n",
    "    You are a helpful assistant, Answer the question                               \n",
    "    based on the context. Keep the answer accurate.                               \n",
    "    Respond \"Unsure about answer\" if not sure about                               \n",
    "    the answer.\n",
    "    \"\"\"                    \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8、配置UserProxy代理\n",
    "User Proxy代理包括一个独特的功能：function_map参数，此参数用于将函数调用的配置与实际函数本身链接起来，确保无缝集成和操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_proxy = autogen.UserProxyAgent(              \n",
    "    name=\"user_proxy\",            \n",
    "    human_input_mode=\"NEVER\",             \n",
    "    max_consecutive_auto_reply=10,            \n",
    "    code_execution_config={\"work_dir\": \"coding\"},            \n",
    "    # llm_config_assistant = llm_config_assistant,            \n",
    "    function_map={                \n",
    "        \"answer_PDF_question\": answer_PDF_question            \n",
    "        }        \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一旦设置了代理，该脚本就会启动用户和聊天机器人之间的对话。这是通过调用user_proxy对象上的initiate_chat方法来完成的。initiate_chat方法需要两个参数：充当聊天机器人的assistant实例和描述任务的文本消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_proxy.initiate_chat(\n",
    "    assistant,    \n",
    "    message=\"\"\"\n",
    "    Write a Openchat word blog post titled why openchat better than GPT3 that uses the exact keyword OpenChat \n",
    "    at least once every 100 words. The blog post should include an introduction, main body, and conclusion. \n",
    "    The conclusion should invite readers to leave a comment. The main body should be split into at least 4 \n",
    "    different subsections.\n",
    "    \"\"\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
