{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用Mistral-7b、LangChain和ChromaDB构建RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG四个主要步骤\n",
    "1. Embedding：使用Embedding模型对文档数据进行embedding操作\n",
    "2. Vector Store：将Embedding后的数据存储到向量数据库中\n",
    "3. Query：将问题embedding后，通过向量数据库检索相关文档\n",
    "4. Answer：使用检索到的文档和问题通过模型生成答案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 安装相关的包\n",
    "```\n",
    "!pip install gradio --quiet\n",
    "!pip install xformer --quiet\n",
    "!pip install chromadb --quiet\n",
    "!pip install langchain --quiet\n",
    "!pip install accelerate --quiet\n",
    "!pip install transformers --quiet\n",
    "!pip install bitsandbytes --quiet\n",
    "!pip install unstructured --quiet\n",
    "!pip install sentence-transformers --quiet\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gradio as gr\n",
    "\n",
    "from textwrap import fill\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    )\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import AIMessage, HumanMessage\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader, UnstructuredURLLoader\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain, RetrievalQA, ConversationalRetrievalChain\n",
    "\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 使用4位加载模型\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # 使用16位计算\n",
    "    bnb_4bit_quant_type=\"nf4\",  # 使用4位量化\n",
    "    bnb_4bit_use_double_quant=True,  # 使用双量化\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)  # 使用fast tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token  # 使用eos作为pad token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    torch_dtype=torch.float16,  # 使用16位浮点数\n",
    "    trust_remote_code=True,  # 信任远程代码\n",
    "    device_map=\"auto\",  # 自动选择设备\n",
    "    quantization_config=quantization_config  # 量化配置\n",
    ")\n",
    "\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)  # 从预训练模型加载生成配置\n",
    "generation_config.max_new_tokens = 1024 \n",
    "generation_config.temperature = 0.0001 \n",
    "generation_config.top_p = 0.95 \n",
    "generation_config.do_sample = True  # 采样\n",
    "generation_config.repetition_penalty = 1.15  # 重复惩罚\n",
    "\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,  # 返回完整文本\n",
    "    generation_config=generation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安装xformer，以实现更高效的内存注意力实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipeline,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Mistral-7b模型进行问答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = \"Explain the difference between ChatGPT and open source LLMs in a couple of lines.\"\n",
    "result = llm(\n",
    "    query\n",
    ")\n",
    "\n",
    "display(Markdown(f\"<b>{query}</b>\"))\n",
    "display(Markdown(f\"<p>{result}</p>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = \"What is Hiberus GenIA Ecosystem?\"\n",
    "result = llm(\n",
    "    query\n",
    ")\n",
    "\n",
    "display(Markdown(f\"<b>{query}</b>\"))\n",
    "display(Markdown(f\"<p>{result}</p>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embedding模型\n",
    "Embedding模型使用阿里巴巴达摩院预训练并在Hugging Face上开源的embedding模型GTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"thenlper/gte-large\",\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prompt Template\n",
    "PromptTemplate通过结构化Prompt格式使模型按照用户期望的格式进行输出，模板可以包括指令、few-shot例子以及适合特定任务的特定上下文和问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "[INST] <>\n",
    "Act as a Machine Learning engineer who is teaching high school students.\n",
    "<>\n",
    "\n",
    "{text} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Explain what are Deep Neural Networks in 2-3 sentences\"\n",
    "result = llm(prompt.format(text=query))\n",
    "\n",
    "display(Markdown(f\"<b>{query}</b>\"))\n",
    "display(Markdown(f\"<p>{result}</p>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 数据加载\n",
    "为了准确回答之前的问题（What is Hiberus GenIA Ecosystem?），必须将LLM与GenIA生态系统的信息联系起来。有两个网页是理解GenIA生态系统的关键。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://www.hiberus.com/expertos-ia-generativa-ld\",\n",
    "    \"https://www.hiberus.com/en/experts-generative-ai-ld\"\n",
    "]\n",
    "\n",
    "loader = UnstructuredURLLoader(urls=urls)\n",
    "documents = loader.load()\n",
    "\n",
    "len(documents)\n",
    "# Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于这两个文档数据量较大，以及超过了Mistral-7b大模型的上下文窗口大小，因此我们需要将文档按照1024个tokens大小进行切分，生成21个较小的chunks，并且为了保证上下文的连续性，chunk与chunk直接设置64个重叠tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "texts_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "len(texts_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 数据注入\n",
    "对数据分块之后，我们将对分块数据进行embedding并存储到向量数据库Chromdb中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(texts_chunks, embeddings, persist_directory=\"db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据被添加索引之后，我们可以在Prompt模板中添加RAG模型赋予营销经理专家的角色！\n",
    "\n",
    "此外，为了将LLM与矢量数据库检索功能相结合，我们使用了关键的链接组件RetrievalQA，其中k=2。这种设置确保检索器输出两个相关的块，然后LLM在提出问题时使用这两个块来制定答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "[INST] <>\n",
    "Act as an Hiberus marketing manager expert. Use the following information to answer the question at the end.\n",
    "<>\n",
    "\n",
    "{context}\n",
    "\n",
    "{question} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",   \n",
    "    retriever=db.as_retriever(search_kwargs={\"k\": 2}),  # 使用检索器\n",
    "    return_source_documents=True,  # 返回源文档\n",
    "    chain_type_kwargs={\"prompt\": prompt},  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is GenAI Ecosystem?\"\n",
    "result_ = qa_chain(\n",
    "    query\n",
    ")\n",
    "result = result_[\"result\"].strip()\n",
    "\n",
    "\n",
    "display(Markdown(f\"<b>{query}</b>\"))\n",
    "display(Markdown(f\"<p>{result}</p>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Why Hiberus has created GenAI Ecosystem?\"\n",
    "result_ = qa_chain(\n",
    "    query\n",
    ")\n",
    "result = result_[\"result\"].strip()\n",
    "\n",
    "\n",
    "display(Markdown(f\"<b>{query}</b>\"))\n",
    "display(Markdown(f\"<p>{result}</p>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出源文档或参考文档\n",
    "result_[\"source_documents\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 问答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_template = \"\"\"You are an Hiberus Marketing Manager AI Assistant. Given the\n",
    "following conversation and a follow up question, rephrase the follow up question\n",
    "to be a standalone question. At the end of standalone question add this\n",
    "'Answer the question in English language.' If you do not know the answer reply with 'I am sorry, I dont have enough information'.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\n",
    "\"\"\"\n",
    "\n",
    "CUSTOM_QUESTION_PROMPT = PromptTemplate.from_template(custom_template)  # 从模板创建PromptTemplate\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)  # 创建对话缓存内存\n",
    "\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=db.as_retriever(search_kwargs={\"k\": 2}),\n",
    "    memory=memory,\n",
    "    condense_question_prompt=CUSTOM_QUESTION_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who you are?\"\n",
    "result_ = qa_chain({\"question\": query})\n",
    "result = result_[\"answer\"].strip()\n",
    "\n",
    "display(Markdown(f\"<b>{query}</b>\"))\n",
    "display(Markdown(f\"<p>{result}</p>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看聊天记录\n",
    "memory.chat_memory.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 基于Gradio搭建问答UI界面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def querying(query, history):\n",
    "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "  qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "      llm=llm,\n",
    "      retriever=db.as_retriever(search_kwargs={\"k\": 2}),\n",
    "      memory=memory,\n",
    "      condense_question_prompt=CUSTOM_QUESTION_PROMPT,\n",
    "  )\n",
    "\n",
    "  result = qa_chain({\"question\": query})\n",
    "  return result[\"answer\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iface = gr.ChatInterface(\n",
    "    fn = querying,\n",
    "    chatbot=gr.Chatbot(height=600),\n",
    "    textbox=gr.Textbox(placeholder=\"What is GenAI Ecosystem?\", container=False, scale=7),\n",
    "    title=\"HiberusBot\",\n",
    "    theme=\"soft\",\n",
    "    examples=[\"Why Hiberus has created GenAI Ecosystem?\",\n",
    "              \"What is GenAI Ecosystem?\"],\n",
    "\n",
    "    cache_examples=True,\n",
    "    retry_btn=\"Repetir\",\n",
    "    undo_btn=\"Deshacer\",\n",
    "    clear_btn=\"Borrar\",\n",
    "    submit_btn=\"Enviar\"\n",
    "\n",
    "    )\n",
    "\n",
    "iface.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
