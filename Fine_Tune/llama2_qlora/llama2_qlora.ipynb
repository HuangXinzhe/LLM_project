{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llama2 qlora 微调\n",
    "- llama2\n",
    "    - 上下文4096\n",
    "    - 使用了Grouped-Query Attention（GQA）共享多头注意力的key 和value矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets peft accelerate bitsandbytes safetensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import torch\n",
    "import datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    GenerationConfig\n",
    ")\n",
    "from peft import PeftModel, LoraConfig, prepare_model_for_kbit_training, get_peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### config ###\n",
    "model_id = \"NousResearch/Llama-2-7b-hf\" # optional meta-llama/Llama-2–7b-chat-hf\n",
    "max_length = 512\n",
    "device_map = \"auto\"\n",
    "batch_size = 128\n",
    "micro_batch_size = 32\n",
    "gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "\n",
    "# nf4\" use a symmetric quantization scheme with 4 bits precision\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# load model from huggingface\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    use_cache=False,\n",
    "    device_map=device_map\n",
    ")\n",
    "\n",
    "# load tokenizer from huggingface\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输出模型的可训练参数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    print(f\"trainable model parameters: {trainable_model_params}. All model parameters: {all_model_params} \")\n",
    "    return trainable_model_params\n",
    "\n",
    "ori_p = print_number_of_trainable_model_parameters(model)\n",
    "\n",
    "# 输出\n",
    "# trainable model parameter: 262,410,240"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置LoRA参数\n",
    "当lora_alpha的值较高时，表示权重更新的幅度较大，这可能会使模型更容易过拟合特定的任务数据。相反，当lora_alpha的值较低时，权重更新的幅度较小，有助于保持预训练模型的知识，并减少过拟合的风险"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "### compare trainable parameters #\n",
    "peft_p = print_number_of_trainable_model_parameters(model)\n",
    "print(f\"# Trainable Parameter \\nBefore: {ori_p} \\nAfter: {peft_p} \\nPercentage: {round(peft_p / ori_p * 100, 2)}\")\n",
    "\n",
    "# 输出\n",
    "# trainable model parameter: 4,194,304"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 微调前效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### generate ###\n",
    "prompt = \"Write me a poem about Singapore.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=64)\n",
    "print('\\nAnswer: ', tokenizer.decode(generate_ids[0]))\n",
    "res = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要揭秘LLM能力，构建Prompt是至关重要，通常的Prompt形式有三个字段：Instruction、Input（optional）、Response。由于Input是可选的，因为这里设置了两种prompt_template，分别是有Input 的prompt_input和无Input 的prompt_no_input，代码如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256\n",
    "dataset = datasets.load_dataset(\n",
    "    \"databricks/databricks-dolly-15k\", split='train'\n",
    ")\n",
    "\n",
    "### generate prompt based on template ###\n",
    "prompt_template = {\n",
    "    \"prompt_input\": \\\n",
    "    \"Below is an instruction that describes a task, paired with an input that provides further context.\\\n",
    "    Write a response that appropriately completes the request.\\\n",
    "    \\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\",\n",
    "\n",
    "    \"prompt_no_input\": \\\n",
    "    \"Below is an instruction that describes a task.\\\n",
    "    Write a response that appropriately completes the request.\\\n",
    "    \\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n\",\n",
    "\n",
    "    \"response_split\": \"### Response:\"\n",
    "}\n",
    "\n",
    "def generate_prompt(instruction, input=None, label=None, prompt_template=prompt_template):\n",
    "    if input:\n",
    "        res = prompt_template[\"prompt_input\"].format(\n",
    "            instruction=instruction, input=input)\n",
    "    else:\n",
    "        res = prompt_template[\"prompt_no_input\"].format(\n",
    "            instruction=instruction)\n",
    "    if label:\n",
    "        res = f\"{res}{label}\"\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用generate_prompt函数把instruction, context和response拼接起来；然后进行tokenize分词处理，转换为input_ids和attention_mask，为了让模型可以预测下一个token，设计了类似input_ids的labels便于右移操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tokenizer, prompt, max_length=max_length, add_eos_token=False):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=False,\n",
    "        return_tensors=None)\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(\n",
    "        data_point[\"instruction\"],\n",
    "        data_point[\"context\"],\n",
    "        data_point[\"response\"],\n",
    "    )\n",
    "    tokenized_full_prompt = tokenize(tokenizer, full_prompt)\n",
    "    user_prompt = generate_prompt(data_point[\"instruction\"], data_point[\"context\"])\n",
    "    tokenized_user_prompt = tokenize(tokenizer, user_prompt)\n",
    "    user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "    mask_token = [-100] * user_prompt_len\n",
    "    tokenized_full_prompt[\"labels\"] = mask_token + tokenized_full_prompt[\"labels\"][user_prompt_len:]\n",
    "    return tokenized_full_prompt\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=1000, shuffle=True, seed=42)\n",
    "cols = [\"instruction\", \"context\", \"response\", \"category\"]\n",
    "train_data = dataset[\"train\"].shuffle().map(generate_and_tokenize_prompt, remove_columns=cols)\n",
    "val_data = dataset[\"test\"].shuffle().map(generate_and_tokenize_prompt, remove_columns=cols,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./llama-7b-int4-dolly\",\n",
    "    num_train_epochs=20,\n",
    "    max_steps=200,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    per_device_train_batch_size=micro_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    gradient_checkpointing=True,\n",
    "    group_by_length=False,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=args,\n",
    "    data_collator=DataCollatorForSeq2Seq(\n",
    "      tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True),\n",
    ")\n",
    "\n",
    "# silence the warnings. re-enable for inference!\n",
    "model.config.use_cache = False\n",
    "trainer.train()\n",
    "model.save_pretrained(\"llama-7b-int4-dolly\")\n",
    "\n",
    "tokenizer.save_pretrained(\"llama-7b-int4-dolly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model path and weight\n",
    "model_id = \"NousResearch/Llama-2-7b-hf\"\n",
    "peft_path = \"./llama-7b-int4-dolly\"\n",
    "\n",
    "# loading model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    use_cache=False,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# loading peft weight\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    peft_path,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# generation config\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=40,\n",
    "    num_beams=4, # beam search\n",
    ")\n",
    "\n",
    "# generating reply\n",
    "with torch.no_grad():\n",
    "    prompt = \"Write me a poem about Singapore.\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    generation_output = model.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=64,\n",
    "    )\n",
    "    print('\\nAnswer: ', tokenizer.decode(generation_output.sequences[0]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
