{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLHF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预训练\n",
    "- 数据量：100B-5Ttokens\n",
    "- 任务：无标注文本的下一个单词预测\n",
    "## 有监督微调\n",
    "- 数据量：1k-50k\n",
    "- 任务：无标注文本的下一个单词预测\n",
    "- 训练数据：{\"Instruction\": \"\", \"Input\": \"\", \"output\": \"\"}\n",
    "## 对齐\n",
    "- 数据量：大于50k\n",
    "- 任务：将语言模型与人类的偏好、价值观进行对齐\n",
    "\n",
    "RLHF主要包括三步：\n",
    "- 在预训练好的模型上进行「有监督微调」（SFT）；\n",
    "- 在有监督微调模型基础上创建一个reward model（RM）模型；\n",
    "- 基于RM模型使用PPO算法微调SFT模型；\n",
    "\n",
    "llama中RLHF的特别之处：\n",
    "- 第二阶段有有用性和安全性的奖励模型\n",
    "- 拒绝采样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
