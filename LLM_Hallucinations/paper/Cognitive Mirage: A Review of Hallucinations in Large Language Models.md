# Cognitive Mirage: A Review of Hallucinations in Large Language Models
论文链接：https://arxiv.org/abs/2309.06794v1

论文代码：https://github.com/hongbinye/cognitive-mirage-hallucinations-in-llms

## 一、幻觉介绍
幻觉的基本定义：大模型生成看似合理的内容，其实这些内容是不正确的或者是与输入Prompt无关，甚至是有冲突的现象

## 二、幻觉产生的原因
- 数据收集：
    - 预训练数据：大模型的知识和能力主要来自与预训练数据，如果预训练数据使用了不完整或者过期的数据，那么就很可能导致知识的错误，从而引起幻觉现象；
    - 上下文学习：为了让大模型可以更好的输出，有时会在Prompt中增加一些上下文内容，然而这些上下文的类别和pair的顺序也可能引起幻觉，比如前几个example的标签是“是”，后面几个是“否”，那么大模型很可能就输出“否”了；
     - 多语言大模型：处理少语种或者非英文翻译的问题；
- 知识GAP：
    - 主要来自pre-training和fine-tuning阶段的输入数据格式不同引起的。
- 优化过程：
    - 最大似然估计和teacher-forcing训练有可能导致一种被称为随机模仿的现象，大模型在没有真正理解的情况下模仿训练数据，这样可能会导致幻觉；
    - top-k和top-p采样技术也可能导致幻觉，LLM倾向于产生滚雪球般的幻觉，以保持与早期幻觉的一致性，即使在“Let’s think step by step”这样的提示下，它们仍然会产生无效的推理链；
## 三、幻觉的分类
- 推理分类器
    - 给定问题Q和答案A，训练一个分类器，让分类器去判断生成的答案是否包含幻觉。
- 不确定度度量
    - 给定问题Q和答案A，训练一个分类器，让分类器去判断生成的答案是否包含幻觉H
- 自我评估
- 证据检索
- 生成模型
    - 证据检索通过检索与幻觉有关的支持性证据来辅助事实检测
## 四、幻觉解决方案
- 参数自适应
- 事后归因和编辑技术
- 利用外部知识
- 评估反馈
- 心态社会
## 五、幻觉待解决问题
- 数据构建管理
- 下游任务协调
- 推理机制开发
- 多模态幻觉调查