{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AirLLM使用4G显存即可在70B大模型上进行推理\n",
    "- 70B模型需要130G显存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、分层推理（Layer-wise inference）\n",
    "-  在推理过程中，层按顺序执行，上一层的输出是下一层的输入，一次只执行一个层。因此，完全没有必要将所有层都保存在GPU内存中。我们可以在执行该层时从磁盘加载所需的任何层，进行所有计算，然后完全释放内存。这样，每层所需的GPU内存仅为一个transformer层的参数大小，即整个模型的1/80，约1.6GB。\n",
    "- 此外，一些输出缓存也存储在GPU内存中，最大的是KV缓存，以避免重复计算。对于70B模型，这个KV缓存大小大约是：\n",
    "    - 2*input_length*num_layers*num_heads*vector_dim*4\n",
    "    - 输入长度为100时，此缓存=2\\*100\\*80\\*8\\*128\\*4=30MB GPU内存。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、Flash attention\n",
    "- 将计算拆分为多个小块，逐块计算，并将内存减少到一个块的大小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、模型文件共享\n",
    "- 对原始的HuggingFace模型文件进行预处理，并对其进行分层分割。\n",
    "- 对于存储，使用安全张量技术(https://github.com/huggingface/safetensors)。Safetensor确保存储格式和内存中格式紧密匹配，并使用内存映射进行加载以最大限度地提高速度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、元设备（Meta Device）\n",
    "- 使用HuggingFace Accelerate提供的Meta Device功能https://huggingface.co/docs/accelerate/usage\\\\_guides/bigh\\\\_modeling来实施。Meta Device是一种专门为运行超大型模型而设计的虚拟设备。当您通过Meta Device加载模型时，模型数据实际上并没有被读入，只是加载了代码，内存使用率为0。\n",
    "- 在执行过程中，您可以将模型的部分内容从Meta Device动态转移到CPU或GPU等真实设备。只有到那时，它才真正加载到内存中。\n",
    "- 使用init_empty_weights（）可以通过Meta Device加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import init_empty_weights\n",
    "with init_empty_weights():\n",
    "    my_model = ModelClass(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5、开源项目\n",
    "上述所有技术已经集成到AirLLM  \n",
    "https://github.com/lyogavin/airllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install airllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airllm import AutoModel\n",
    "\n",
    "MAX_LENGTH = 128\n",
    "# could use hugging face model repo id:\n",
    "model = AutoModel.from_pretrained(\"garage-bAInd/Platypus2-70B-instruct\")\n",
    "# model = AutoModel.from_pretrained(\"unsloth/Meta-Llama-3.1-405B-Instruct-bnb-4bit\")\n",
    "\n",
    "\n",
    "# or use model's local path...\n",
    "# model = AutoModel.from_pretrained(\"/home/ubuntu/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/b585e74bcaae02e52665d9ac6d23f4d0dbc81a0f\")\n",
    "\n",
    "input_text = [\n",
    "    'What is the capital of United States?',\n",
    "    # 'I like',\n",
    "]\n",
    "\n",
    "input_tokens = model.tokenizer(input_text,\n",
    "                               return_tensors=\"pt\",\n",
    "                               return_attention_mask=False,\n",
    "                               truncation=True,\n",
    "                               max_length=MAX_LENGTH,\n",
    "                               padding=False)\n",
    "\n",
    "generation_output = model.generate(\n",
    "    input_tokens['input_ids'].cuda(),\n",
    "    max_new_tokens=20,\n",
    "    use_cache=True,\n",
    "    return_dict_in_generate=True)\n",
    "\n",
    "output = model.tokenizer.decode(generation_output.sequences[0])\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
