{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT4All\n",
    "GPT4All软件旨在优化运行在笔记本电脑、台式机和服务器CPU上的3-13亿参数的大型语言模型的推理性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gpt4all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt4all import GPT4All\n",
    "model = GPT4All(\"orca-mini-3b.ggmlv3.q4_0.bin\")\n",
    "output = model.generate(\"The capital of France is \", max_tokens=3)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "聊天"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT4All(model_name='orca-mini-3b.ggmlv3.q4_0.bin')\n",
    "with model.chat_session():\n",
    "    response = model.generate(prompt='hello', top_k=1)\n",
    "    response = model.generate(prompt='write me a short poem', top_k=1)\n",
    "    response = model.generate(prompt='thank you', top_k=1)\n",
    "    print(model.current_chat_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 当在chat_session上下文中运行GPT4All模型时，模型将提供一个与聊天相关的提示模板，同时保留了先前对话历史的内部K/V缓存，从而提升了推理速度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT4All的generate()函数接收以下参数：\n",
    "\n",
    "prompt (str)：作为模型的提示语。\n",
    "\n",
    "max_tokens (int)：生成的最大令牌数。\n",
    "\n",
    "temp (float)：模型的“温度”。值越大，生成的文本越有创意，但可能偏离事实。\n",
    "\n",
    "top_k (int)：在每次生成步骤中，从最可能的top_k个令牌中进行随机选取。设置为1以执行贪婪解码。\n",
    "\n",
    "top_p (float)：在每次生成步骤中，从总概率为top_p的最可能的令牌中进行随机选取。\n",
    "\n",
    "repeat_penalty (float)：对模型重复的惩罚。值越高，生成的文本重复程度越低。\n",
    "\n",
    "repeat_last_n (int)：指定在模型生成历史中应用重复惩罚的距离。\n",
    "\n",
    "n_batch (int)：并行处理的提示令牌数量。增加此值可以减少延迟，但会增加资源需求。\n",
    "\n",
    "n_predict (Optional[int])：与max_tokens功能相同，主要为了向后兼容。\n",
    "\n",
    "streaming (bool)：如果设置为True，此方法将返回一个生成器，此生成器会在模型生成令牌时产生。\n",
    "\n",
    "       GPT4All还支持流式生成，您可以在生成时使用streaming = True参数与GPT4All的响应进行交互。当在聊天会话中流式传输令牌时，您需要手动处理聊天历史的收集和更新。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT4All Python Embeddings\n",
    "GPT4All提供了使用CPU优化的对比训练的句子转换器生成不受长度限制的文本文档的高级嵌入的支持。这些嵌入的质量在众多任务上与OpenAI的相媲美"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt4all import GPT4All, Embed4All\n",
    "text = 'The quick brown fox jumps over the lazy dog'\n",
    "embedder = Embed4All()\n",
    "output = embedder.embed(text)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
