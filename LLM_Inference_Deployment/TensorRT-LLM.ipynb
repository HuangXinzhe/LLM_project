{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorRT-LLM\n",
    "https://nvidia.github.io/TensorRT-LLM/  \n",
    "\n",
    "英伟达(NVIDIA)在TensorRT基础上针对LLM优化所推出的推理加速引擎TensorRT-LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安装\n",
    "https://github.com/NVIDIA/TensorRT-LLM/blob/release/0.5.0/docs/source/installation.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用\n",
    "使用TensorRT-LLM部署大模型大致分为如下三个步骤：\n",
    "- 下载预训练模型权重；\n",
    "- 创建大模型的全优化引擎；\n",
    "- 部署该引擎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step0：在docker容器中安装所需要的环境\n",
    "```\n",
    "pip install -r examples/bloom/requirements.txt\n",
    "git lfs install\n",
    "```\n",
    "Step1：从Huggingface中下载BLOOM-650m模型\n",
    "```\n",
    "cd examples/bloom\n",
    "rm -rf ./bloom/560M\n",
    "mkdir -p ./bloom/560M && git clone https://huggingface.co/bigscience/bloom-560m ./bloom/560M\n",
    "```\n",
    "Step2：创建引擎\n",
    "```\n",
    "# Single GPU on BLOOM 560M\n",
    "python build.py --model_dir ./bloom/560M/ \\\n",
    "                --dtype float16 \\\n",
    "                --use_gemm_plugin float16 \\\n",
    "                --use_gpt_attention_plugin float16 \\\n",
    "                --output_dir ./bloom/560M/trt_engines/fp16/1-gpu/\n",
    "```\n",
    "Note：关于参数的细节可以参考https://github.com/NVIDIA/TensorRT-LLM/blob/release/0.5.0/examples/bloom  \n",
    "Step3：运行引擎\n",
    "```\n",
    "python summarize.py --test_trt_llm \\\n",
    "                    --hf_model_location ./bloom/560M/ \\\n",
    "                    --data_type fp16 \\\n",
    "                    --engine_dir ./bloom/560M/trt_engines/fp16/1-gpu/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrtllm as trtllm\n",
    "\n",
    "# Initialize the model\n",
    "model = trtllm.LargeLanguageModel('./path_to_your_model')\n",
    "\n",
    "# Apply kernel fusion and quantization\n",
    "optimization_flags = trtllm.OptimizationFlag.FUSE_OPERATIONS | trtllm.OptimizationFlag.QUANTIZE\n",
    "optimized_model = model.optimize(flags=optimization_flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable in-flight batching and paged attention\n",
    "runtime_parameters = {\n",
    "'in_flight_batching': True,\n",
    "'paged_attention': True\n",
    "}\n",
    "\n",
    "# Build the engine with these runtime optimizations\n",
    "engine = optimized_model.build_engine(runtime_parameters=runtime_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [...] # your input data here\n",
    "results = engine.execute_with_inflight_batching(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 适配多种类型的LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrtllm as trtllm\n",
    " \n",
    "# Define and load different LLMs\n",
    "llama_model = trtllm.LargeLanguageModel('./path_to_llama_model')\n",
    "chatglm_model = trtllm.LargeLanguageModel('./path_to_chatglm_model')\n",
    "\n",
    "# Build optimized engines for different LLMs\n",
    "llama_engine = llama_model.build_engine()\n",
    "chatglm_engine = chatglm_model.build_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 降低硬件资源依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrtllm as trtllm\n",
    " \n",
    "# Initialize the model\n",
    "model = trtllm.LargeLanguageModel('./path_to_your_model')\n",
    "\n",
    "# Optimize the model with energy-efficient settings\n",
    "optimized_model = model.optimize(energy_efficient=True)\n",
    "\n",
    "# Monitor energy consumption\n",
    "energy_usage = optimized_model.monitor_energy_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单实用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrtllm as trtllm\n",
    " \n",
    "# Initialize and load the model\n",
    "model = trtllm.LargeLanguageModel('./path_to_your_model')\n",
    "\n",
    "# Perform common operations through easy-to-understand methods\n",
    "model.optimize()\n",
    "model.build_engine()\n",
    "model.execute(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrtllm as trtllm\n",
    " \n",
    "# Initialize the model\n",
    "model = trtllm.LargeLanguageModel('./path_to_your_model')\n",
    "\n",
    "# Enable quantization\n",
    "quantized_model = model.enable_quantization(precision='FP8')\n",
    "\n",
    "# Build and execute the quantized model\n",
    "engine = quantized_model.build_engine()\n",
    "result = engine.execute(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 适应新架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrtllm as trtllm\n",
    " \n",
    "# Initialize the model\n",
    "model = trtllm.LargeLanguageModel('./path_to_your_model')\n",
    "\n",
    "# Update the model with new kernels or architectures\n",
    "updated_model = model.update_components(new_kernels='./path_to_new_kernels',\n",
    "                                    new_architectures='./path_to_new_architectures')\n",
    "\n",
    "# Re-optimize and deploy the updated model\n",
    "updated_engine = updated_model.build_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
